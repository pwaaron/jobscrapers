{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping jobs on Careers@Gov\n",
    "Careers@Gov is a career portal to become a public servant in the Government of Singapore.\n",
    "This career portal is relevant to get jobs in the government, which is usually not posted in normal web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the job URLs\n",
    "First, we will collect all the URLs from the search pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "N_JOBS = 1832 #Number of available jobs on the portal. Change accordingly.\n",
    "PAGES = N_JOBS // 20 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index starts from 1\n",
    "for i in range(1, PAGES + 1):\n",
    "    search_page = requests.get('https://careers.pageuppeople.com/688/cwlive/en/listing/?page={}'.format(i))\n",
    "    search_page_soup = BeautifulSoup(search_page.text, 'lxml')\n",
    "    links.extend(list(map(lambda link: \"https://careers.pageuppeople.com\" + link[\"href\"], search_page_soup.find_all(\"a\", {\"class\": \"job-link\"}))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the job pages\n",
    "After collecting the URLs, download the job pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(set(links))\n",
    "pages = []\n",
    "got_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(link):\n",
    "    try:\n",
    "        req = requests.get(link)\n",
    "        got_links.append(link)\n",
    "        return req\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(filter(lambda page: page, map(getPage, links)))\n",
    "link = got_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the job pages into Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(map(lambda response: BeautifulSoup(response.text, 'lxml'), pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get information from job pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all organisations are stated. We have a failsafe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOrg(txt):\n",
    "    try:\n",
    "        ans = txt.find(\"div\", {\"class\": \"jobDetails\"}).find_all(\"span\")[1].text\n",
    "        return ans\n",
    "    except: \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = list(map(lambda txt: txt.find(\"div\", {\"id\": \"job-content\"}).find(\"h2\").text, texts))\n",
    "organisation = list(map(lambda txt: findOrg(txt), texts))\n",
    "job_code = list(map(lambda txt: txt.find(\"span\", {\"class\": \"job-externalJobNo\"}).text, texts))\n",
    "work_type = list(map(lambda txt: txt.find(\"span\", {\"class\": \"work-type\"}).text, texts))\n",
    "location = list(map(lambda txt: txt.find(\"span\", {\"class\": \"location\"}).text, texts))\n",
    "category = list(map(lambda txt: txt.find(\"span\", {\"class\": \"categories\"}).text, texts))\n",
    "job_description = list(map(lambda txt: txt.find(\"div\", {\"id\": \"job-details\"}).text, texts))\n",
    "date_retrieved = list(map(lambda txt: txt.find(\"span\", {\"class\": \"open-date\"}).text, texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = {'Job Title':job_title, 'Organisation': organisation, \n",
    "      'Job Code': job_code, 'work_type': work_type,\n",
    "      'Location': location, 'Category': category,\n",
    "      'Job Description': job_description, 'Date Retrieved': date_retrieved,\n",
    "      'Link': link}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.DataFrame(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.to_csv('careergov.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
